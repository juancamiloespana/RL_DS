{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Descripción del problema de la mochila\n",
        "\n",
        "Considere el caso en el cual usted dispone de una mochila con una capacidad máxima $b$, debiendo decir dentro de un conjunto de objetos, cada uno con una peso y valor dado, cuales debe empacar con el fin de maximizar el valor de los objetos en la mochila. La siguiente tabla presenta los datos para una instancia del problema en el que se asume que la mochila tiene una capacidad de 100 unidades\n",
        "\n",
        "| Item  | 1  | 2  | 3  | 4  | 5  | 6  | 7  | 8 | 9  | 10 | 11 | 12 | 13 | 14 | 15 |\n",
        "|-------|----|----|----|----|----|----|----|---|----|----|----|----|----|----|----|\n",
        "| Valor | 15 | 80 | 90 | 60 | 40 | 15 | 95 | 8 | 45 | 50 | 60 | 35 | 32 | 27 | 61 |\n",
        "| peso  | 7  | 20 | 25 | 18 | 15 | 5  | 30 | 4 | 14 | 17 | 19 | 12 | 12 | 10 | 17 |\n",
        "\n",
        "<center>¿Cuáles items deberán empacarse en la mochila?</center>\n",
        "\n",
        "---\n",
        "\n",
        "Este problema puede formularse de la siguiente forma\n",
        "\n",
        "## Conjuntos\n",
        "> $I$: Conjunto de items\n",
        "\n",
        "## Parámetros\n",
        "> $v_{i}$: Valor de cada item $i$ \n",
        "\n",
        "> $w_i$: peso de cada item $i$\n",
        "\n",
        "> $C$: Capacidad de la mochila\n",
        "\n",
        "## Variables de decisión\n",
        "\n",
        "> \\begin{equation}\n",
        "    x_i=\n",
        "    \\begin{cases}\n",
        "      1, & \\text{si se selecciona el item i} \\\\\n",
        "      0, & \\text{en otro caso}\n",
        "    \\end{cases}\n",
        "  \\end{equation}\n",
        "\n",
        "## Función objetivo\n",
        "\n",
        "> $$\\text{maximizar}\\  \\sum_{i \\in I}v_{i}x_{i} $$\n",
        "\n",
        "## Restricciones\n",
        "\n",
        "**1. Capacidad de la mochila**\n",
        "\n",
        "> $\\sum_{i \\in I}  w_ix_i \\leq C$\n",
        "\n",
        "\n",
        "**2. restricción de dominio de las variables**\n",
        "\n",
        "Expresa que todas las variables deben ser no negativas. Es decir:\n",
        "> $x_{i} \\in \\{0,1\\} \\ \\forall i \\in I$ "
      ],
      "metadata": {
        "id": "kbSBdJVF-MxT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datos de entrada"
      ],
      "metadata": {
        "id": "_-dKzRRe-ZG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ITEMS = {1, 2, 3, 4, 5, 6, 7,\t8, 9, 10, 11, 12, 13, 14, 15},\n",
        "#profits = {1: 15, 2: 80, 3: 90 , 4: 60 , 5: 40, 6: 15, 7: 95,\t8: 8, \n",
        "#                   9: 45, 10: 50, 11: 60, 12: 35, 13: 32, 14: 27, 15: 61}\n",
        "#weights = {1: 7, 2: 20, 3: 25 , 4: 18 , 5: 15, 6: 5, 7: 30,\t8: 4, \n",
        "#                   9: 14, 10: 17, 11: 19, 12: 12, 13: 12, 14: 10, 15: 17}\n",
        "#capacity = 100\n",
        "\n",
        "ITEMS = [1, 2, 3, 4, 5, 6, 7,\t8, 9, 10, 11, 12, 13, 14, 15]\n",
        "n_items = len(ITEMS)\n",
        "values = [15, 80, 90, 60, 40, 15, 95, 8, 45, 50, 60, 35, 32, 27, 61]\n",
        "weights = [7, 20, 25, 18, 15, 5, 30, 4, 14, 17, 19, 12, 12, 10, 17]\n",
        "capacity = 100"
      ],
      "metadata": {
        "id": "9HD8JN4w-bwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo de aprendizaje reforzado "
      ],
      "metadata": {
        "id": "7xX6wwKH1KXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "El modelo formulado considera los siguientes elementos\n",
        "* Un entorno (`environment`) del que en cada instante del tiempo se puede obtener una observación (`obs`) de su estado actual. \n",
        "* Una observación corresponde a un vector de `n_items + 2` posiciones, donde `n_items` representa el número de items disponibles para empacar en la mochila y las dos posiciones adicionales corresponden, respectivamente, al valor de los items actuelmente empacados y a la capacidad remanente en la mochila . Un ejemplo de la observación del sistema para un problema con 5 items sería: \n",
        "> `[ 0 1 0 1 0 17 20]`\n",
        "* Un agente (`agent`) que con base en una observación del estado actual del sistema toma una acción `a` respecto a que item de los que no han sido empacados debe adicionarse a la mochila\n",
        "* La decisión del agente se basa en la predicción realizada por una red neuronal (`net`). Esta red recibe una observación y devuelve la probabilidad con que cada item deberia ser seleccionado\n",
        "\n"
      ],
      "metadata": {
        "id": "MGKTRdj7PKRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementaremos cada uno de los elementos mencionados, para ello primero instalamos los pquetes requeridos"
      ],
      "metadata": {
        "id": "78jmrOQzRXtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalar librerias y paquetes"
      ],
      "metadata": {
        "id": "xoPv0-kY1hpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "Opiaq0L21MzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Red neuronal para predecir acción "
      ],
      "metadata": {
        "id": "vzZz1qNf1CwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "aQAXBA6b4i97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probemos la red con una observación fictica"
      ],
      "metadata": {
        "id": "f1EP85-eSE2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos la estructura interna de la red\n",
        "HIDDEN_SIZE = 128\n",
        "\n",
        "# Creamos la red, definimos el tipo de oprimización y suavizamos la salida\n",
        "net = Net(n_items+2, HIDDEN_SIZE, n_items)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "sm = nn.Softmax(dim=1)\n",
        "\n",
        "# Creamos una observación cualquiera y calculamos las probabilidades de cada item para ser empacado\n",
        "obs = [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 85]\n",
        "obs_v = torch.FloatTensor([obs])\n",
        "act_probs_v = sm(net(obs_v))\n",
        "act_probs_v\n",
        "act_probs = act_probs_v.data.numpy()[0]\n",
        "act_probs\n"
      ],
      "metadata": {
        "id": "KAm2DK0A5quZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clase `environment`"
      ],
      "metadata": {
        "id": "a_EnZ4XnTiwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class environment:\n",
        "  '''    \n",
        "    Attributes:\n",
        "    n (int): number of items \n",
        "    capacity (real): knapsck capcity\n",
        "    values (list:int): Value of each item\n",
        "    weights (list:int): Weight of each item\n",
        "    iters (int): Number of iterations in each try to solve the problem\n",
        "                 it can be set equal to the number of items\n",
        "\n",
        "  '''\n",
        "\n",
        "  def __init__(self, n, capacity, values, weights, iters):\n",
        "        \n",
        "        self.n_items = n # number of items\n",
        "        self.decision = np.zeros(self.n_items)   # 1 for items selected\n",
        "        self.values = values # Value of each item\n",
        "        self.weights = weights  # Weight of each item            \n",
        "        self.iters = iters # Number of iterations        \n",
        "        self.n_moves = 0    # Search steps counter        \n",
        "        self.kp_value = 0 # Total knapsack value\n",
        "        self.capacity = capacity # knapsck remanent capacity\n",
        "        self.obs = self.decision.tolist() # observation \n",
        "        self.obs.append(self.kp_value)\n",
        "        self.obs.append(self.capacity)\n",
        "        \n",
        "  def step(self, a):\n",
        "    \"\"\" Takes an action and updates the environment. \"\"\"\n",
        "    is_done = False # Indicates if the limit of iterations is reached\n",
        "    step_valid = False # Indicates if an action is feasible     \n",
        "    self.n_moves += 1 # update number of tried moves\n",
        "\n",
        "    # update decisions vector if the movement is feasible\n",
        "    if self.decision[a] == 0 and (self.weights[a] <= self.capacity):\n",
        "      step_valid = True\n",
        "      self.decision[a] = 1\n",
        "      self.kp_value += self.values[a] \n",
        "      self.capacity -= self.weights[a]\n",
        "\n",
        "      # Update observation\n",
        "      self.obs = self.decision.tolist() \n",
        "      self.obs.append(self.kp_value)\n",
        "      self.obs.append(self.capacity)\n",
        "\n",
        "    else:\n",
        "      is_done = True  \n",
        "     \n",
        "    # Verify if the limit of iterations is reached\n",
        "    if self.n_moves >= self.iters:\n",
        "      is_done = True   \n",
        "        \n",
        "    return step_valid, is_done\n",
        "    \n",
        "  def reset(self):\n",
        "    \"\"\" Resets results while keeping settings\"\"\"\n",
        "    self.decision = np.zeros(self.n_items) \n",
        "    self.n_moves = 0 \n",
        "    self.kp_value = 0\n",
        "    self.capacity = capacity\n",
        "    self.obs = self.decision.tolist() \n",
        "    self.obs.append(self.kp_value)\n",
        "    self.obs.append(self.capacity)"
      ],
      "metadata": {
        "id": "kvsx9YG9JDp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clase `agent`"
      ],
      "metadata": {
        "id": "z1ThOihYVpkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class agent:\n",
        "  '''    \n",
        "    Attributes:\n",
        "    sm (Softmax): softmax operator \n",
        "    eps (real [0,1]): controls diversity\n",
        "    values (list:int): Value of each item\n",
        "    weights (list:int): Weight of each item\n",
        "    iters (int): Number of iterations in each try to solve the problem\n",
        "                 it can be set equal to the number of items\n",
        "\n",
        "  '''\n",
        "  def __init__(self, sm, eps):    \n",
        "    self.action = -99 # Define null action\n",
        "    self.sm = sm    \n",
        "    self.eps = eps\n",
        "\n",
        "\n",
        "  def choose_action(self, env, net):\n",
        "    '''Obtains and observation from the environment and chooses and \n",
        "    action based on the probabilities given for a neural network'''    \n",
        "    p = np.random.rand() # Generate random number\n",
        "    # Takes firs decision randomly \n",
        "    if env.decision.sum() == 0:      \n",
        "      self.action = np.random.choice(env.n_items)\n",
        "    elif p < self.eps:\n",
        "      # Randomly select an action\n",
        "      self.action = np.random.choice(env.n_items)\n",
        "    else:\n",
        "      # Take greedy action chosing the one with highest probability\n",
        "      obs_v = torch.FloatTensor([env.obs])\n",
        "      act_probs_v = self.sm(net(obs_v))\n",
        "      act_probs = act_probs_v.data.numpy()[0]      \n",
        "      self.action = np.random.choice(len(act_probs), p=act_probs)  \n",
        "\n",
        "    \n",
        "    return self.action\n",
        "\n"
      ],
      "metadata": {
        "id": "eYPK4OaETMVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iteración en batches"
      ],
      "metadata": {
        "id": "vpQa0ZOxWvj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# usa namedtuples para guardar los pasos de cada episodio y los episodios \n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "\n",
        "\n",
        "def iterate_batches(env, agent, net, batch_size):\n",
        "  '''Generate batches of knapsack solutions. Each solution consist of the \n",
        "     steps = [observation, actions] taken to solve the problem. \n",
        "     The batch collects several solutions (batch_size) '''  \n",
        "  batch = []\n",
        "  episode_reward = 0.0\n",
        "  episode_steps = []\n",
        "  sm = nn.Softmax(dim=1)\n",
        "  env.reset()\n",
        "\n",
        "  # Repeat until enough solutions are built\n",
        "  while True:\n",
        "    a = agent.choose_action(env, net)     \n",
        "    step_valid, is_done = env.step(a)\n",
        "    if step_valid:\n",
        "      step = EpisodeStep(observation=env.obs, action=a)\n",
        "      episode_steps.append(step) \n",
        "    # if a solutions is complete, it reset the environment to start a new solution   \n",
        "    if is_done:\n",
        "      e = Episode(reward=env.kp_value, steps=episode_steps)\n",
        "      batch.append(e)\n",
        "      env.reset()\n",
        "      episode_steps = []\n",
        "      # If enough solutions are generated it returns the batch\n",
        "      if len(batch) == batch_size:\n",
        "        yield batch\n",
        "        batch = []"
      ],
      "metadata": {
        "id": "rn_t-M8uv1I1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecciona las mejores soluciones "
      ],
      "metadata": {
        "id": "FGrynS1fZCfR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escoge las mejores soluciones de un batch y extrae las observaciones y la acción que el agente tomó en ellas"
      ],
      "metadata": {
        "id": "mBZ2eKZsZIQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_batch(batch, percentile):\n",
        "    '''Selects the best solutions given a percentile  ''' \n",
        "    rewards = list(map(lambda s: s.reward, batch))\n",
        "    reward_bound = np.percentile(rewards, percentile)\n",
        "    position_best = np.argmax(rewards)\n",
        "    reward_max = rewards[position_best]    \n",
        "\n",
        "    train_obs = []\n",
        "    train_act = []\n",
        "    for reward, steps in batch:\n",
        "        if reward < reward_bound:\n",
        "            continue\n",
        "        train_obs.extend(map(lambda step: step.observation, steps))\n",
        "        train_act.extend(map(lambda step: step.action, steps))\n",
        "    train_obs_v = torch.FloatTensor(train_obs)\n",
        "    train_act_v = torch.LongTensor(train_act)\n",
        "    \n",
        "    return train_obs_v, train_act_v, reward_bound, reward_max"
      ],
      "metadata": {
        "id": "zXMhTuR5TeRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimentación"
      ],
      "metadata": {
        "id": "VXl28KpBawNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Una instancia pequeña"
      ],
      "metadata": {
        "id": "FxRChjJ3fpcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "PERCENTILE = 70\n",
        "N_EPISODES = 100\n",
        "EPSILON = 0.01\n",
        "\n",
        "# Lists to save the observed solutions\n",
        "incunbent_hist =[0]\n",
        "incunbent = 0\n",
        "best_list = [0]\n",
        "\n",
        "# Create neural network \n",
        "net = Net(n_items+2, HIDDEN_SIZE, n_items)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "sm = nn.Softmax(dim=1)\n",
        "# Create  agent and environment\n",
        "env = environment(len(ITEMS), capacity, values, weights, len(ITEMS))\n",
        "ag = agent(sm, EPSILON)\n",
        "\n",
        "\n",
        "# Iterate over batches. Each time a batch is created the neural network is\n",
        "# updated\n",
        "for iter_no, batch in enumerate(iterate_batches(env, ag, net, BATCH_SIZE)):\n",
        "  \n",
        "  # Filters the batch\n",
        "  obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
        "  \n",
        "  # Updates the incumbent (best solution)\n",
        "  if reward_m > incunbent:\n",
        "    incunbent = reward_m\n",
        "  incunbent_hist.append(incunbent)\n",
        "  best_list.append(reward_m)\n",
        "\n",
        "  # Update network\n",
        "  optimizer.zero_grad()\n",
        "  action_scores_v = net(obs_v)\n",
        "  loss_v = objective(action_scores_v, acts_v)\n",
        "  loss_v.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # stops if the number of episodes is reached\n",
        "  if iter_no >= N_EPISODES:\n",
        "    break\n"
      ],
      "metadata": {
        "id": "4oJQLHuOV45D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print solutions"
      ],
      "metadata": {
        "id": "54SH6Klka2lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "fig = go.Figure()\n",
        "x = list(range(N_EPISODES))\n",
        "fig.add_trace(go.Scatter(x = x, y = incunbent_hist, name= \"incumbent\"))\n",
        "fig.add_trace(go.Scatter(x = x, y = best_list, name= \"best_episode\"))\n",
        " \n",
        "fig.show()"
      ],
      "metadata": {
        "id": "rQM0dZkqY4tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Una instancia grande\n",
        "\n",
        "Leemos los datos"
      ],
      "metadata": {
        "id": "QMDsFq9Rfs3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "\n",
        "!gdown 1eiR-X0leK1qAM2EuulKW2wdYyP2owDX4 \n",
        "\n",
        "df_instance = pd.read_excel('knapPI_2_100_1000_1.xlsx')\n",
        "df_instance\n",
        "ITEMS = df_instance[\"id\"].values.tolist()\n",
        "n_items = len(ITEMS)\n",
        "values = df_instance[\"value\"].values.tolist()\n",
        "weights = df_instance[\"weight\"].values.tolist()\n",
        "capacity = 1000"
      ],
      "metadata": {
        "id": "tA8HMAX7ctCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Corremos el algoritmo"
      ],
      "metadata": {
        "id": "FhMs4Bagf7d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "PERCENTILE = 70\n",
        "N_EPISODES = 10000\n",
        "EPSILON = 0.01\n",
        "\n",
        "# Lists to save the observed solutions\n",
        "incunbent_hist =[0]\n",
        "incunbent = 0\n",
        "best_list = [0]\n",
        "\n",
        "# Create neural network \n",
        "net = Net(n_items+2, HIDDEN_SIZE, n_items)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "sm = nn.Softmax(dim=1)\n",
        "# Create  agent and environment\n",
        "env = environment(len(ITEMS), capacity, values, weights, len(ITEMS))\n",
        "ag = agent(sm, EPSILON)\n",
        "\n",
        "\n",
        "# Iterate over batches. Each time a batch is created the neural network is\n",
        "# updated\n",
        "for iter_no, batch in enumerate(iterate_batches(env, ag, net, BATCH_SIZE)):\n",
        "  \n",
        "  # Filters the batch\n",
        "  obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
        "  \n",
        "  # Updates the incumbent (best solution)\n",
        "  if reward_m > incunbent:\n",
        "    incunbent = reward_m\n",
        "  incunbent_hist.append(incunbent)\n",
        "  best_list.append(reward_m)\n",
        "\n",
        "  # Update network\n",
        "  optimizer.zero_grad()\n",
        "  action_scores_v = net(obs_v)\n",
        "  loss_v = objective(action_scores_v, acts_v)\n",
        "  loss_v.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # stops if the number of episodes is reached\n",
        "  if iter_no >= N_EPISODES:\n",
        "    break"
      ],
      "metadata": {
        "id": "c6sYtHMdd9VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráficamos la solución"
      ],
      "metadata": {
        "id": "lzQcxcY2f91X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "fig = go.Figure()\n",
        "x = list(range(N_EPISODES))\n",
        "fig.add_trace(go.Scatter(x = x, y = incunbent_hist, name= \"incumbent\"))\n",
        "fig.add_trace(go.Scatter(x = x, y = best_list, name= \"best_episode\"))\n",
        " \n",
        "fig.show()"
      ],
      "metadata": {
        "id": "xCuic4i5eRl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación con `gym`"
      ],
      "metadata": {
        "id": "HjWtMCiPAOXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos las librerias necesarias"
      ],
      "metadata": {
        "id": "ioSuU5tDX1Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "mD3EAAvsJ_gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clase environmenmt"
      ],
      "metadata": {
        "id": "7X2bE5N3Y3hU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el entorno"
      ],
      "metadata": {
        "id": "6pD1ec09X4Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnapsackEnv(gym.Env):\n",
        "  def __init__(self, n, capacity, values, weights, iters):\n",
        "    self.n_items = n # number of items\n",
        "    self.values = values # Value of each item\n",
        "    self.weights = weights  # Weight of each item \n",
        "    self.capacity = capacity # knapsck  capacity\n",
        "    self.iters = iters # Number of iterations\n",
        "    self.n_moves = 0    # Search steps counter \n",
        "\n",
        "    # action: select an item to include in the knapsack\n",
        "    #self.action_space = spaces.Discrete(self.n_items) \n",
        "    \n",
        "    \n",
        "    # Observations are dictionaries with the agent's and the target's location.\n",
        "    #self.observation_space = spaces.Dict(\n",
        "    #  {\n",
        "    #    \"status\": spaces.Box(0, 1, shape=(self.n_items,), dtype=int),\n",
        "    #    \"kp_value\": spaces.Box(low=0, high=sum(self.values), shape=(1,), dtype=np.float32),\n",
        "    #    \"rem_capacity\": spaces.Box(low=0, high=self.capacity, shape=(1,), dtype=np.float32)\n",
        "    #  })\n",
        "    \n",
        "    self.observation = self.reset()\n",
        "    \n",
        "\n",
        "  def get_obs(self):\n",
        "    #obs = list(self.observation[\"status\"])\n",
        "    #obs.append(self.observation[\"kp_value\"])\n",
        "    #obs.append(self.observation[\"rem_capacity\"])\n",
        "    #return obs\n",
        "    return {\"status\": self.observation[\"status\"], \n",
        "            \"kp_value\": self.observation[\"kp_value\"],\n",
        "            \"rem_capacity\": self.observation[\"rem_capacity\"]}\n",
        "\n",
        "  def flatten_obs(self):\n",
        "    obs = list(self.observation[\"status\"])\n",
        "    obs.append(self.observation[\"kp_value\"])\n",
        "    obs.append(self.observation[\"rem_capacity\"])\n",
        "    return obs\n",
        "  \n",
        "\n",
        "  def reset(self, seed=None, options=None):\n",
        "    observation = {\"status\" : np.zeros(self.n_items), # Nome item selected \n",
        "                   \"kp_value\" : 0, # Total knapsack value\n",
        "                   \"rem_capacity\" : self.capacity } # knapsck  capacity\n",
        "    self.n_moves = 0    # Search steps counter     \n",
        "\n",
        "    self.observation = observation\n",
        "\n",
        "    return observation\n",
        "\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    is_done = False # Indicates if the limit of iterations is reached\n",
        "    step_valid = False # Indicates if an action is feasible\n",
        "    self.n_moves += 1    # Search steps counter   \n",
        "\n",
        "    # update status vector if the action is feasible    \n",
        "    if self.observation[\"status\"][action] == 0 and (self.weights[action] <= self.observation[\"rem_capacity\"]):\n",
        "      \n",
        "      step_valid = True\n",
        "      # status = self.observation[\"status\"]\n",
        "      # status[action] = 1\n",
        "      self.observation[\"status\"][action] = 1\n",
        "      #self.observation[\"status\"] = status\n",
        "      self.observation[\"kp_value\"] += self.values[action] \n",
        "      self.observation[\"rem_capacity\"] -= self.weights[action]\n",
        "\n",
        "    else:\n",
        "      is_done = True  \n",
        "\n",
        "    # Verify if the limit of iterations is reached\n",
        "    if self.n_moves >= self.iters:\n",
        "      is_done = True   \n",
        "\n",
        "    obs = self.get_obs()\n",
        "\n",
        "    return obs, obs[\"kp_value\"], is_done, step_valid\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "C-5-EGfiAl4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clase Agent\n",
        "Creamos el agente\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Awb2nVOJYBMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class agent:\n",
        "  '''    \n",
        "    Attributes:\n",
        "    sm (Softmax): softmax operator \n",
        "    eps (real [0,1]): controls diversity\n",
        "    values (list:int): Value of each item\n",
        "    weights (list:int): Weight of each item\n",
        "    iters (int): Number of iterations in each try to solve the problem\n",
        "                 it can be set equal to the number of items\n",
        "\n",
        "  '''\n",
        "  def __init__(self, sm, eps):    \n",
        "    self.action = -99 # Define null action\n",
        "    self.sm = sm    \n",
        "    self.eps = eps\n",
        "\n",
        "\n",
        "  def choose_action(self, env, net):\n",
        "    '''Obtains and observation from the environment and chooses and \n",
        "    action based on the probabilities given for a neural network'''    \n",
        "    p = np.random.rand() # Generate random number\n",
        "    # Takes firs decision randomly \n",
        "    if env.observation[\"status\"].sum() == 0:      \n",
        "      self.action = np.random.choice(env.n_items)\n",
        "    elif p < self.eps:\n",
        "      # Randomly select an action\n",
        "      self.action = np.random.choice(env.n_items)\n",
        "    else:\n",
        "      # Take greedy action chosing the one with highest probability\n",
        "      obs_v = torch.FloatTensor([env.flatten_obs()])\n",
        "      act_probs_v = self.sm(net(obs_v))\n",
        "      act_probs = act_probs_v.data.numpy()[0]      \n",
        "      self.action = np.random.choice(len(act_probs), p=act_probs)  \n",
        "\n",
        "    \n",
        "    return self.action"
      ],
      "metadata": {
        "id": "-uiylOBSCUV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Red neuronal\n",
        "\n",
        "Creamos la red neuronal que usará el agente"
      ],
      "metadata": {
        "id": "QpbSATTYYEi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "32JVvLZKKg2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iteración en batches"
      ],
      "metadata": {
        "id": "hXf6U93FYK-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# usa namedtuples para guardar los pasos de cada episodio y los episodios \n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "\n",
        "\n",
        "def iterate_batches(env, agent, net, batch_size):\n",
        "  '''Generate batches of knapsack solutions. Each solution consist of the \n",
        "     steps = [observation, actions] taken to solve the problem. \n",
        "     The batch collects several solutions (batch_size) '''  \n",
        "  batch = []\n",
        "  episode_reward = 0.0\n",
        "  episode_steps = []\n",
        "  sm = nn.Softmax(dim=1)\n",
        "  obs = env.reset()\n",
        "\n",
        "  # Repeat until enough solutions are built\n",
        "  while True:\n",
        "    a = agent.choose_action(env, net)     \n",
        "    next_obs, kp_value, is_done, step_valid, = env.step(a)\n",
        "    if step_valid:\n",
        "      step = EpisodeStep(observation=env.flatten_obs(), action=a)\n",
        "      episode_steps.append(step) \n",
        "    # if a solutions is complete, it reset the environment to start a new solution   \n",
        "    if is_done:\n",
        "      e = Episode(reward=env.observation[\"kp_value\"], steps=episode_steps)\n",
        "      batch.append(e)\n",
        "      env.reset()\n",
        "      episode_steps = []\n",
        "      # If enough solutions are generated it returns the batch\n",
        "      if len(batch) == batch_size:\n",
        "        yield batch\n",
        "        batch = []"
      ],
      "metadata": {
        "id": "xwELy64SK1ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecciona mejores soluciones"
      ],
      "metadata": {
        "id": "Q1rG7OpuYQIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_batch(batch, percentile):\n",
        "    '''Selects the best solutions given a percentile  ''' \n",
        "    rewards = list(map(lambda s: s.reward, batch))\n",
        "    reward_bound = np.percentile(rewards, percentile)\n",
        "    position_best = np.argmax(rewards)\n",
        "    reward_max = rewards[position_best]    \n",
        "\n",
        "    train_obs = []\n",
        "    train_act = []\n",
        "    for reward, steps in batch:\n",
        "        if reward < reward_bound:\n",
        "            continue\n",
        "        train_obs.extend(map(lambda step: step.observation, steps))\n",
        "        train_act.extend(map(lambda step: step.action, steps))\n",
        "    train_obs_v = torch.FloatTensor(train_obs)\n",
        "    train_act_v = torch.LongTensor(train_act)\n",
        "    \n",
        "    return train_obs_v, train_act_v, reward_bound, reward_max"
      ],
      "metadata": {
        "id": "cRE-cNouNLUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experimentación"
      ],
      "metadata": {
        "id": "uC2ME9KlYTqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leemos los datos "
      ],
      "metadata": {
        "id": "bAu2QRskYnAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ITEMS = {1, 2, 3, 4, 5, 6, 7,\t8, 9, 10, 11, 12, 13, 14, 15},\n",
        "#profits = {1: 15, 2: 80, 3: 90 , 4: 60 , 5: 40, 6: 15, 7: 95,\t8: 8, \n",
        "#                   9: 45, 10: 50, 11: 60, 12: 35, 13: 32, 14: 27, 15: 61}\n",
        "#weights = {1: 7, 2: 20, 3: 25 , 4: 18 , 5: 15, 6: 5, 7: 30,\t8: 4, \n",
        "#                   9: 14, 10: 17, 11: 19, 12: 12, 13: 12, 14: 10, 15: 17}\n",
        "#capacity = 100\n",
        "\n",
        "ITEMS = [1, 2, 3, 4, 5, 6, 7,\t8, 9, 10, 11, 12, 13, 14, 15]\n",
        "n_items = len(ITEMS)\n",
        "values = [15, 80, 90, 60, 40, 15, 95, 8, 45, 50, 60, 35, 32, 27, 61]\n",
        "weights = [7, 20, 25, 18, 15, 5, 30, 4, 14, 17, 19, 12, 12, 10, 17]\n",
        "capacity = 100"
      ],
      "metadata": {
        "id": "dvGvAyYNQIp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Corremos la instancia"
      ],
      "metadata": {
        "id": "qEFU9Js0YrBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "PERCENTILE = 70\n",
        "N_EPISODES = 100\n",
        "EPSILON = 0.01\n",
        "iters = 10\n",
        "\n",
        "# Lists to save the observed solutions\n",
        "incunbent_hist =[0]\n",
        "incunbent = 0\n",
        "best_list = [0]\n",
        "\n",
        "# Create neural network \n",
        "net = Net(n_items+2, HIDDEN_SIZE, n_items)\n",
        "objective = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "sm = nn.Softmax(dim=1)\n",
        "# Create  agent and environment\n",
        "env = KnapsackEnv(n_items, capacity, values, weights, iters)\n",
        "ag = agent(sm, EPSILON)\n",
        "\n",
        "\n",
        "# Iterate over batches. Each time a batch is created the neural network is\n",
        "# updated\n",
        "for iter_no, batch in enumerate(iterate_batches(env, ag, net, BATCH_SIZE)):\n",
        "  print(iter_no)\n",
        "  # Filters the batch\n",
        "  obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
        "  \n",
        "  # Updates the incumbent (best solution)\n",
        "  if reward_m > incunbent:\n",
        "    incunbent = reward_m\n",
        "  incunbent_hist.append(incunbent)\n",
        "  best_list.append(reward_m)\n",
        "\n",
        "  # Update network\n",
        "  optimizer.zero_grad()\n",
        "  action_scores_v = net(obs_v)\n",
        "  loss_v = objective(action_scores_v, acts_v)\n",
        "  loss_v.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  # stops if the number of episodes is reached\n",
        "  if iter_no >= N_EPISODES:\n",
        "    break"
      ],
      "metadata": {
        "id": "-y1DsWAUu1EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficamos la solución "
      ],
      "metadata": {
        "id": "KX1MwyA7YYsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "fig = go.Figure()\n",
        "x = list(range(N_EPISODES))\n",
        "fig.add_trace(go.Scatter(x = x, y = incunbent_hist, name= \"incumbent\"))\n",
        "fig.add_trace(go.Scatter(x = x, y = best_list, name= \"best_episode\"))\n",
        " \n",
        "fig.show()"
      ],
      "metadata": {
        "id": "-FW5O4glTgHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}